{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b223cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”Ž Running Sequential model tuning...\n",
      "Sequential Model 1: Layers=[10], LR=0.001, Dropout=0.0\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "Sequential Model 2: Layers=[10], LR=0.001, Dropout=0.3\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "Sequential Model 3: Layers=[10], LR=0.01, Dropout=0.0\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "Sequential Model 4: Layers=[10], LR=0.01, Dropout=0.3\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "Sequential Model 5: Layers=[16, 8], LR=0.001, Dropout=0.0\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "Sequential Model 6: Layers=[16, 8], LR=0.001, Dropout=0.3\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "Sequential Model 7: Layers=[16, 8], LR=0.01, Dropout=0.0\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "Sequential Model 8: Layers=[16, 8], LR=0.01, Dropout=0.3\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "Sequential Model 9: Layers=[32, 16], LR=0.001, Dropout=0.0\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "Sequential Model 10: Layers=[32, 16], LR=0.001, Dropout=0.3\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "Sequential Model 11: Layers=[32, 16], LR=0.01, Dropout=0.0\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "Sequential Model 12: Layers=[32, 16], LR=0.01, Dropout=0.3\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "Sequential Model 13: Layers=[64, 32], LR=0.001, Dropout=0.0\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "Sequential Model 14: Layers=[64, 32], LR=0.001, Dropout=0.3\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "Sequential Model 15: Layers=[64, 32], LR=0.01, Dropout=0.0\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "Sequential Model 16: Layers=[64, 32], LR=0.01, Dropout=0.3\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "âœ… Sequential tuning complete. Best model saved.\n",
      "ðŸ”Ž Running Wide & Deep model tuning...\n",
      "Wide&Deep Model 1: Layers=[10], LR=0.001, Dropout=0.0\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "Wide&Deep Model 2: Layers=[10], LR=0.001, Dropout=0.3\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "Wide&Deep Model 3: Layers=[10], LR=0.01, Dropout=0.0\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "Wide&Deep Model 4: Layers=[10], LR=0.01, Dropout=0.3\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "Wide&Deep Model 5: Layers=[16, 8], LR=0.001, Dropout=0.0\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "Wide&Deep Model 6: Layers=[16, 8], LR=0.001, Dropout=0.3\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "Wide&Deep Model 7: Layers=[16, 8], LR=0.01, Dropout=0.0\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "Wide&Deep Model 8: Layers=[16, 8], LR=0.01, Dropout=0.3\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "âœ… Wide&Deep tuning complete. Best model saved.\n"
     ]
    }
   ],
   "source": [
    "# 03-hyperparameter-tuning.ipynb\n",
    "\n",
    "# 1. Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import itertools\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import os\n",
    "\n",
    "# 2. Load processed data\n",
    "train = pd.read_csv(\"processed_data/train.csv\")\n",
    "val = pd.read_csv(\"processed_data/val.csv\")\n",
    "test = pd.read_csv(\"processed_data/test.csv\")\n",
    "\n",
    "X_train, y_train = train.drop(\"G3_binary\", axis=1), train[\"G3_binary\"]\n",
    "X_val, y_val = val.drop(\"G3_binary\", axis=1), val[\"G3_binary\"]\n",
    "X_test, y_test = test.drop(\"G3_binary\", axis=1), test[\"G3_binary\"]\n",
    "\n",
    "# Class weights\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weights_dict = {0: class_weights[0], 1: class_weights[1]}\n",
    "\n",
    "\n",
    "# 3. Sequential Model Hyperparameter Tuning\n",
    "hidden_layers_options = [[10], [16,8], [32,16], [64,32]]\n",
    "learning_rates = [0.001, 0.01]\n",
    "dropout_rates = [0.0, 0.3]\n",
    "seq_results = []\n",
    "\n",
    "def build_seq(layers_config, lr, dropout):\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Input(shape=(X_train.shape[1],)))\n",
    "    for units in layers_config:\n",
    "        model.add(layers.Dense(units, activation='relu'))\n",
    "        if dropout > 0:\n",
    "            model.add(layers.Dropout(dropout))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=lr),\n",
    "                  loss=\"binary_crossentropy\",\n",
    "                  metrics=[\"accuracy\", keras.metrics.AUC(name=\"auc\")])\n",
    "    return model\n",
    "\n",
    "print(\"ðŸ”Ž Running Sequential model tuning...\")\n",
    "run_num = 1\n",
    "for layers_config, lr, dr in itertools.product(hidden_layers_options, learning_rates, dropout_rates):\n",
    "    print(f\"Sequential Model {run_num}: Layers={layers_config}, LR={lr}, Dropout={dr}\")\n",
    "    model = build_seq(layers_config, lr, dr)\n",
    "    model.fit(X_train, y_train,\n",
    "              validation_data=(X_val, y_val),\n",
    "              epochs=30, batch_size=32, verbose=0,\n",
    "              class_weight=class_weights_dict)\n",
    "    \n",
    "    probs = model.predict(X_test).ravel()\n",
    "    preds = (probs > 0.5).astype(int)\n",
    "    \n",
    "    seq_results.append({\n",
    "        \"Model No.\": run_num,\n",
    "        \"Layers\": str(layers_config),\n",
    "        \"Learning Rate\": lr,\n",
    "        \"Dropout\": dr,\n",
    "        \"Accuracy\": accuracy_score(y_test, preds),\n",
    "        \"Precision\": precision_score(y_test, preds, zero_division=0),\n",
    "        \"Recall\": recall_score(y_test, preds, zero_division=0),\n",
    "        \"F1-Score\": f1_score(y_test, preds, zero_division=0),\n",
    "        \"ROC-AUC\": roc_auc_score(y_test, probs)\n",
    "    })\n",
    "    run_num += 1\n",
    "\n",
    "os.makedirs(\"results\", exist_ok=True)\n",
    "\n",
    "seq_df = pd.DataFrame(seq_results)\n",
    "seq_df.to_csv(\"results/sequential_hyperparameter_results.csv\", index=False)\n",
    "\n",
    "# Select best Sequential model (prioritise F1, then ROC-AUC)\n",
    "best_seq = seq_df.sort_values(by=[\"F1-Score\",\"ROC-AUC\"], ascending=False).iloc[0]\n",
    "with open(\"results/best_params_sequential.json\", \"w\") as f:\n",
    "    json.dump(best_seq.to_dict(), f)\n",
    "\n",
    "print(\"âœ… Sequential tuning complete. Best model saved.\")\n",
    "\n",
    "\n",
    "# 4. Wide & Deep Model Hyperparameter Tuning\n",
    "cat_cols = [col for col in X_train.columns if any(prefix in col for prefix in \n",
    "    ['school_', 'sex_', 'address_', 'famsize_', 'Pstatus_', 'Mjob_', 'Fjob_', \n",
    "     'reason_', 'guardian_', 'schoolsup_', 'famsup_', 'paid_', 'activities_', \n",
    "     'nursery_', 'higher_', 'internet_', 'romantic_'])]\n",
    "num_cols = [col for col in X_train.columns if col not in cat_cols]\n",
    "\n",
    "X_train_wide, X_val_wide, X_test_wide = X_train[cat_cols], X_val[cat_cols], X_test[cat_cols]\n",
    "X_train_deep, X_val_deep, X_test_deep = X_train[num_cols], X_val[num_cols], X_test[num_cols]\n",
    "\n",
    "param_grid = [\n",
    "    {\"layers\":[[10]], \"lr\":[0.001,0.01], \"dropout\":[0.0,0.3]},\n",
    "    {\"layers\":[[16,8]], \"lr\":[0.001,0.01], \"dropout\":[0.0,0.3]}\n",
    "]\n",
    "\n",
    "wd_results = []\n",
    "\n",
    "def build_wd(hidden_layers, lr, dropout):\n",
    "    input_wide = layers.Input(shape=(X_train_wide.shape[1],), name=\"wide_input\")\n",
    "    input_deep = layers.Input(shape=(X_train_deep.shape[1],), name=\"deep_input\")\n",
    "    \n",
    "    deep = input_deep\n",
    "    for units in hidden_layers:\n",
    "        deep = layers.Dense(units, activation='relu')(deep)\n",
    "        if dropout > 0:\n",
    "            deep = layers.Dropout(dropout)(deep)\n",
    "    \n",
    "    combined = layers.concatenate([input_wide, deep])\n",
    "    output = layers.Dense(1, activation='sigmoid')(combined)\n",
    "    model = keras.Model(inputs=[input_wide, input_deep], outputs=output)\n",
    "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=lr),\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy', keras.metrics.AUC(name='auc')])\n",
    "    return model\n",
    "\n",
    "print(\"ðŸ”Ž Running Wide & Deep model tuning...\")\n",
    "run_num = 1\n",
    "for g in param_grid:\n",
    "    for combo in itertools.product(g[\"layers\"], g[\"lr\"], g[\"dropout\"]):\n",
    "        layers_config, lr, dr = combo\n",
    "        print(f\"Wide&Deep Model {run_num}: Layers={layers_config}, LR={lr}, Dropout={dr}\")\n",
    "        model = build_wd(layers_config, lr, dr)\n",
    "        model.fit([X_train_wide, X_train_deep], y_train,\n",
    "                  validation_data=([X_val_wide, X_val_deep], y_val),\n",
    "                  epochs=30, batch_size=32, verbose=0,\n",
    "                  class_weight=class_weights_dict)\n",
    "        \n",
    "        probs = model.predict([X_test_wide, X_test_deep]).ravel()\n",
    "        preds = (probs > 0.5).astype(int)\n",
    "        \n",
    "        wd_results.append({\n",
    "            \"Model No.\": run_num,\n",
    "            \"Layers\": str(layers_config),\n",
    "            \"Learning Rate\": lr,\n",
    "            \"Dropout\": dr,\n",
    "            \"Accuracy\": accuracy_score(y_test, preds),\n",
    "            \"Precision\": precision_score(y_test, preds, zero_division=0),\n",
    "            \"Recall\": recall_score(y_test, preds, zero_division=0),\n",
    "            \"F1-Score\": f1_score(y_test, preds, zero_division=0),\n",
    "            \"ROC-AUC\": roc_auc_score(y_test, probs)\n",
    "        })\n",
    "        run_num += 1\n",
    "\n",
    "wd_df = pd.DataFrame(wd_results)\n",
    "wd_df.to_csv(\"results/widendeep_hyperparameter_results.csv\", index=False)\n",
    "\n",
    "# Select best Wide & Deep model\n",
    "best_wd = wd_df.sort_values(by=[\"F1-Score\",\"ROC-AUC\"], ascending=False).iloc[0]\n",
    "with open(\"results/best_params_widendeep.json\", \"w\") as f:\n",
    "    json.dump(best_wd.to_dict(), f)\n",
    "\n",
    "print(\"âœ… Wide&Deep tuning complete. Best model saved.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
